{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Part II - PyTorch\n",
    "\n",
    "The library we've been using on our examples so far, **scikit-learn**, offers many options of \"canned\" Machine Learning models via an interface where different algorithms can be manipulated as simple objects. It is a great way to get started with Machine Learning and even solve real-life problems using simple to moderately complex models, with moderately large amounts of data.\n",
    "\n",
    "However, to tackle more data-intensive tasks you need more powerful tools and **scikit-learn's** canned models won't be enough.\n",
    "\n",
    "This is where **PyTorch** comes in. It has many performance advantages over **scikit-learn**, including but not limited to:\n",
    "\n",
    "1. It supports GPU acceleration whereas sklearn only supports CPU.\n",
    "\n",
    "2. It explicitly requires you to define and control the class of functions (we'll call it Models for now on) you'll use to approximate the Target, the loss functions and the optimization solvers as separate objects (some may argue this is an inconvenience). This allows more flexibility to pick the right combination of tools for each type of problem.\n",
    "\n",
    "3. Its interface for defining Models is more flexible and expressive than sklearn's. This enables you to easily create very complex models. \n",
    "\n",
    "4. It has user-friendly frameworks for parallel and distributed model training.\n",
    "\n",
    "5. It offers out-of-the-box class templates that make it easy to deal with massive datasets.\n",
    "\n",
    "**PyTorch** can be seen as a general purpose Machine Learning, or even Scientific Computing framework. However, here we focus on the most common usage of the library: building Neural Networks.\n",
    "\n",
    "# Neural Networks Primer:\n",
    "\n",
    "If you look beyond the cool name, you will see that Neural Networks are just another \"family\" of mathematical function $f(X;\\theta)$. But this family of functions is so useful that it sometimes seems almost magical. To add to their allure, Neural Networks can be difficult to write down mathematically depending on how complex their \"architeture\" is. It is easier to visualize how they work by drawing them as a graph:\n",
    "\n",
    "![](./images/nnet.png)\n",
    "\n",
    "Where each neuron represents a linear combination of its inputs with a non-linear function applied to it:\n",
    "\n",
    "![](./images/neuron.png)\n",
    "\n",
    "The **weights** $W$ of each neuron in each layer are the parameters $\\theta$ of this class of function.\n",
    "\n",
    "Common choices of non-linear **activation** functions for neurons are the *Sigmoid (or Logistic)* function:\n",
    "\n",
    "$f(x) = \\frac{1}{1+\\exp{-x}}$\n",
    "\n",
    "And the Rectified Linear Unit (a.k.a. ReLU):\n",
    "\n",
    "$f(x) = \\max{(0,x)}$\n",
    "\n",
    "The type of neural network represented above is known as a **Feed Forward Neural Network** and it is the most \"basic\" member of this family of functions. More complex Neural Networks include ones where neurons in one layer do not necessarily connect to all neurons in the next layer; where other mathematical operations, like *convolutions* take place inside a neuron; and where ouputs in one layer can become inputs of preceding layers.\n",
    "\n",
    "As we've seen, Machine Learning algorithms generally involve solving the optimization problem\n",
    "\n",
    "$\\hat{F(X)} = \\underset{\\theta}{\\operatorname{argmin}} L(Y,f(X;\\theta))$\n",
    "\n",
    "This generally requires finding critical points of a function, that is, points where the derivative of the function with respect to its parameters $\\theta$ is zero.\n",
    "\n",
    "Whether you attempt to do it analytically or numerically... How do you compute the derivatives of $L$ with respect to $\\theta$ when there's a crazy Neural Network nested in it?\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "For reasons we will not delve into in this workshop, an efficient way of solving the optimization problem above is by combining two numerical algorithms: **Backpropagation** and variants of **Gradient Descent**.\n",
    "\n",
    "Here is a summary of the method with vanilla Gradient Descent:\n",
    "\n",
    "1. Randomly initialize all Weights $W$ of the neural network.\n",
    "\n",
    "2. Forward Propagation: Run your inputs X through the neural neutwork and get an output $\\hat{Y}$.\n",
    "\n",
    "3. Take the output of the Neural Network and compute the Loss $L(\\hat{Y}, Y)$.\n",
    "\n",
    "4. Backpropagation: Run the computed loss value backwards through all layers of the network, but this time each layer will represent the derivatives of the loss function with respect to the **Weights** in that layer. At the end of this process, you will obtain an estimate of $\\nabla_W L$, the gradient of $L$ with respect to all **Weights** of the neural network.\n",
    "    \n",
    "5. Gradient Descent - Update the **Weights** $W$ using the gradient computed in step 4: $W = W - \\alpha\\nabla_W L$, where $\\alpha$ is a (usually small) constant called the **learning rate**. \n",
    "\n",
    "6. Repeat steps 1 through 5 until a stopage criterion is reached. Common criteria include:\n",
    "\n",
    "    a. The Loss reaches zero, or a value smaller than a pre-defined threshold.\n",
    "    \n",
    "    b. Steps 1 through 5 have been repeated a (usually large) pre-defined number of times.\n",
    "    \n",
    "Variations of this method include, but are not limited to:\n",
    "\n",
    "* Running all examples X at once through the network, resulting in large matrix multiplications being performed; \n",
    "\n",
    "* Running smaller, *randomly selected batches* of examples X through the network instead; \n",
    "\n",
    "* Using an adaptive learning rate;\n",
    "\n",
    "* Using other types of weight update (also called \"a step\").\n",
    "\n",
    "* Randomly selecting neurons to be *dropped out* from the computations at each iteration;\n",
    "\n",
    "# Loss Function\n",
    "\n",
    "You know the Loss function is a measure of the error we incurr in when using a function $f(x;\\theta)$ to approximate a target $F^*$, but we haven't seen what it looks like. The choice of an appropriate Loss function will depend first on the type of task at hand, then on statistical properties of the data and, to a smaller degree, on the choice of algorithm. Below are some common choices of Loss function.\n",
    "\n",
    "### Task: Regression\n",
    "\n",
    "**Mean Squared Error (MSE) Loss:**\n",
    "\n",
    "$L(Y,\\hat{Y}) = \\frac{1}{N}\\sum{(Y-\\hat{Y})^2}$\n",
    "\n",
    "**Mean Absolute Error (MAE) Loss:**\n",
    "\n",
    "$L(Y,\\hat{Y}) =\\frac{1}{N}\\sum{|Y-\\hat{Y}|}$\n",
    "\n",
    "### Task: Classification\n",
    "\n",
    "**Cross Entropy Loss:**\n",
    "\n",
    "$L(Y,\\hat{Y}) = -\\sum{Y_{class} * log(\\hat{Y_{class}})}$\n",
    "\n",
    "This Loss is commonly used in Classification problems where there are more than two classes of outputs.\n",
    "\n",
    "**The Binary Cross Entropy Loss:**\n",
    "\n",
    "$L(Y,\\hat{Y}) = -(Y*log(\\hat{Y}) + (1-Y)*log(1-\\hat{Y}))$\n",
    "\n",
    "You may recognize this as the general Cross Entropy above with only two output classes, or the negative log-likelihood of a Bernoulli Distribution.\n",
    "\n",
    "____\n",
    "\n",
    "## Neural Networks With PyTorch Example 1 - Iris Dataset Revisited\n",
    "    \n",
    "To see all this in action, let's go back to the Iris Dataset and train a logistic regression model with **PyTorch**, this time representing it as a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "# LET'S CREATE OUR TRAINING AND TEST SETS\n",
    "\n",
    "header = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species']\n",
    "\n",
    "iris_dataset = read_csv('./data/iris.csv',names = header) \n",
    "\n",
    "# ENCODE SPECIES AS CATEGORY NUMBERS \n",
    "iris_dataset.loc[iris_dataset.species=='Iris-setosa', 'species'] = 0\n",
    "iris_dataset.loc[iris_dataset.species=='Iris-versicolor', 'species'] = 1\n",
    "iris_dataset.loc[iris_dataset.species=='Iris-virginica', 'species'] = 2\n",
    "\n",
    "X = iris_dataset.values[:,0:4].astype('float32')\n",
    "Y = iris_dataset.values[:,4].astype('int32')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "\n",
    "# CONVERT DATASETS TO PYTORCH TENSORS\n",
    "X_train = Variable(torch.Tensor(X_train).float())\n",
    "X_test = Variable(torch.Tensor(X_test).float())\n",
    "Y_train= Variable(torch.Tensor(Y_train).long())\n",
    "Y_test = Variable(torch.Tensor(Y_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (fc1): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# DEFINE OUR LOGISTIC REGRESSION MODEL AS A NEURAL NETWORK, INITIALIZE AN OPTIMIZER AND PICK A LOSS FUNCTION\n",
    "\n",
    "\n",
    "# THERE IS A WAY TO CALL MODELS AS FUNCTIONS LIKE WE DID WITH SKLEARN, BUT CLASSES ARE PREFERABLE\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(4, 3)\n",
    "    def forward(self, X):\n",
    "        X = self.fc1(X)\n",
    "\n",
    "        return X  \n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "loss_function = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "# NOW WE TRAIN THE MODEL\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # FORWARD-PROPAGATION     \n",
    "    Y_hat = model(X_train)\n",
    "\n",
    "    loss = loss_function(Y_hat, Y_train)\n",
    "    #print(\"Loss at step\", i, \"is:\" , loss.data.item())\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    loss.backward()\n",
    "\n",
    "    # UPDATE WEIGHTS\n",
    "    optimizer.step()\n",
    "    \n",
    "# NOTICE HOW THE ERROR DECREASES WITH EACH STEP\n",
    "\n",
    "print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This model got 70.0 % right\n"
     ]
    }
   ],
   "source": [
    "# HOW DID IT DO ON THE TEST SET?\n",
    "\n",
    "Y_hat_test = model(X_test)\n",
    "Y_predicted = torch.max(Y_hat_test, 1).indices\n",
    "\n",
    "print(\"\\n This model got\", accuracy_score(Y_test, Y_predicted)*100, \"% right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=4, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# NOW WHAT HAPPENS IF WE USE A MORE COMPLEX NEURAL NET INSTEAD OF LOGISTIC REGRESSION?\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(4, 100)\n",
    "        self.fc2 = torch.nn.Linear(100, 100) # NOTICE THE SIZE OF THE OUTPUT MATCHES THE SIZE OF THE INPUT OF THE NEXT LAYER\n",
    "        self.fc3 = torch.nn.Linear(100, 3)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = torch.nn.functional.sigmoid(self.fc1(X)) # NOTICE THE SIGMOID ACTIVATION APPLIED TO EACH LAYER\n",
    "        X = torch.nn.functional.sigmoid(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "nnet_model = NeuralNet()\n",
    "\n",
    "nnet_optimizer = torch.optim.SGD(nnet_model.parameters(), lr=learning_rate, momentum = 0.9) # ADDING A MOMENTUM TERM TO THE WEIGHT UPDATES  \n",
    "nnet_loss_function = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "print(nnet_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is: 1.1090209484100342\n",
      "Loss at step 1 is: 1.10553777217865\n",
      "Loss at step 2 is: 1.1000306606292725\n",
      "Loss at step 3 is: 1.0943821668624878\n",
      "Loss at step 4 is: 1.0902012586593628\n",
      "Loss at step 5 is: 1.08830988407135\n",
      "Loss at step 6 is: 1.0885721445083618\n",
      "Loss at step 7 is: 1.0901042222976685\n",
      "Loss at step 8 is: 1.0917283296585083\n",
      "Loss at step 9 is: 1.0924698114395142\n",
      "Loss at step 10 is: 1.0918768644332886\n",
      "Loss at step 11 is: 1.090077519416809\n",
      "Loss at step 12 is: 1.0876057147979736\n",
      "Loss at step 13 is: 1.085112452507019\n",
      "Loss at step 14 is: 1.083101749420166\n",
      "Loss at step 15 is: 1.0817910432815552\n",
      "Loss at step 16 is: 1.0811043977737427\n",
      "Loss at step 17 is: 1.0807735919952393\n",
      "Loss at step 18 is: 1.080474615097046\n",
      "Loss at step 19 is: 1.0799486637115479\n",
      "Loss at step 20 is: 1.0790702104568481\n",
      "Loss at step 21 is: 1.077858567237854\n",
      "Loss at step 22 is: 1.0764434337615967\n",
      "Loss at step 23 is: 1.0749964714050293\n",
      "Loss at step 24 is: 1.0736602544784546\n",
      "Loss at step 25 is: 1.0725065469741821\n",
      "Loss at step 26 is: 1.0715234279632568\n",
      "Loss at step 27 is: 1.07063627243042\n",
      "Loss at step 28 is: 1.0697535276412964\n",
      "Loss at step 29 is: 1.0688040256500244\n",
      "Loss at step 30 is: 1.0677598714828491\n",
      "Loss at step 31 is: 1.0666327476501465\n",
      "Loss at step 32 is: 1.065459132194519\n",
      "Loss at step 33 is: 1.064278244972229\n",
      "Loss at step 34 is: 1.0631159543991089\n",
      "Loss at step 35 is: 1.061979055404663\n",
      "Loss at step 36 is: 1.0608588457107544\n",
      "Loss at step 37 is: 1.0597407817840576\n",
      "Loss at step 38 is: 1.0586109161376953\n",
      "Loss at step 39 is: 1.0574615001678467\n",
      "Loss at step 40 is: 1.0562913417816162\n",
      "Loss at step 41 is: 1.0551016330718994\n",
      "Loss at step 42 is: 1.0538970232009888\n",
      "Loss at step 43 is: 1.0526787042617798\n",
      "Loss at step 44 is: 1.051446795463562\n",
      "Loss at step 45 is: 1.05020010471344\n",
      "Loss at step 46 is: 1.0489352941513062\n",
      "Loss at step 47 is: 1.0476514101028442\n",
      "Loss at step 48 is: 1.0463478565216064\n",
      "Loss at step 49 is: 1.0450270175933838\n",
      "Loss at step 50 is: 1.0436904430389404\n",
      "Loss at step 51 is: 1.0423389673233032\n",
      "Loss at step 52 is: 1.0409709215164185\n",
      "Loss at step 53 is: 1.0395840406417847\n",
      "Loss at step 54 is: 1.038173794746399\n",
      "Loss at step 55 is: 1.0367369651794434\n",
      "Loss at step 56 is: 1.0352717638015747\n",
      "Loss at step 57 is: 1.0337779521942139\n",
      "Loss at step 58 is: 1.0322576761245728\n",
      "Loss at step 59 is: 1.0307133197784424\n",
      "Loss at step 60 is: 1.0291458368301392\n",
      "Loss at step 61 is: 1.0275547504425049\n",
      "Loss at step 62 is: 1.0259382724761963\n",
      "Loss at step 63 is: 1.0242937803268433\n",
      "Loss at step 64 is: 1.0226181745529175\n",
      "Loss at step 65 is: 1.020909070968628\n",
      "Loss at step 66 is: 1.0191657543182373\n",
      "Loss at step 67 is: 1.0173875093460083\n",
      "Loss at step 68 is: 1.0155751705169678\n",
      "Loss at step 69 is: 1.0137299299240112\n",
      "Loss at step 70 is: 1.0118510723114014\n",
      "Loss at step 71 is: 1.0099382400512695\n",
      "Loss at step 72 is: 1.0079901218414307\n",
      "Loss at step 73 is: 1.0060051679611206\n",
      "Loss at step 74 is: 1.0039805173873901\n",
      "Loss at step 75 is: 1.001914620399475\n",
      "Loss at step 76 is: 0.9998071193695068\n",
      "Loss at step 77 is: 0.9976568222045898\n",
      "Loss at step 78 is: 0.9954642057418823\n",
      "Loss at step 79 is: 0.9932280778884888\n",
      "Loss at step 80 is: 0.9909486174583435\n",
      "Loss at step 81 is: 0.9886245131492615\n",
      "Loss at step 82 is: 0.9862549901008606\n",
      "Loss at step 83 is: 0.9838380217552185\n",
      "Loss at step 84 is: 0.9813733696937561\n",
      "Loss at step 85 is: 0.9788591265678406\n",
      "Loss at step 86 is: 0.9762946963310242\n",
      "Loss at step 87 is: 0.9736787676811218\n",
      "Loss at step 88 is: 0.9710110425949097\n",
      "Loss at step 89 is: 0.9682912230491638\n",
      "Loss at step 90 is: 0.9655182361602783\n",
      "Loss at step 91 is: 0.9626911878585815\n",
      "Loss at step 92 is: 0.9598093032836914\n",
      "Loss at step 93 is: 0.9568712115287781\n",
      "Loss at step 94 is: 0.9538761973381042\n",
      "Loss at step 95 is: 0.9508239030838013\n",
      "Loss at step 96 is: 0.9477128386497498\n",
      "Loss at step 97 is: 0.9445427060127258\n",
      "Loss at step 98 is: 0.9413134455680847\n",
      "Loss at step 99 is: 0.938023030757904\n",
      "Loss at step 100 is: 0.9346725344657898\n",
      "Loss at step 101 is: 0.9312607049942017\n",
      "Loss at step 102 is: 0.9277871251106262\n",
      "Loss at step 103 is: 0.9242512583732605\n",
      "Loss at step 104 is: 0.920652449131012\n",
      "Loss at step 105 is: 0.9169909954071045\n",
      "Loss at step 106 is: 0.9132661819458008\n",
      "Loss at step 107 is: 0.9094782471656799\n",
      "Loss at step 108 is: 0.9056268334388733\n",
      "Loss at step 109 is: 0.9017118811607361\n",
      "Loss at step 110 is: 0.8977335691452026\n",
      "Loss at step 111 is: 0.8936918377876282\n",
      "Loss at step 112 is: 0.8895872831344604\n",
      "Loss at step 113 is: 0.8854203820228577\n",
      "Loss at step 114 is: 0.8811903595924377\n",
      "Loss at step 115 is: 0.8768991231918335\n",
      "Loss at step 116 is: 0.8725467920303345\n",
      "Loss at step 117 is: 0.8681339025497437\n",
      "Loss at step 118 is: 0.8636615872383118\n",
      "Loss at step 119 is: 0.859130859375\n",
      "Loss at step 120 is: 0.85454261302948\n",
      "Loss at step 121 is: 0.8498979806900024\n",
      "Loss at step 122 is: 0.8451982140541077\n",
      "Loss at step 123 is: 0.8404454588890076\n",
      "Loss at step 124 is: 0.8356401324272156\n",
      "Loss at step 125 is: 0.8307850360870361\n",
      "Loss at step 126 is: 0.8258812427520752\n",
      "Loss at step 127 is: 0.820931077003479\n",
      "Loss at step 128 is: 0.8159360289573669\n",
      "Loss at step 129 is: 0.8108986020088196\n",
      "Loss at step 130 is: 0.8058212399482727\n",
      "Loss at step 131 is: 0.8007057905197144\n",
      "Loss at step 132 is: 0.7955553531646729\n",
      "Loss at step 133 is: 0.7903719544410706\n",
      "Loss at step 134 is: 0.7851584553718567\n",
      "Loss at step 135 is: 0.7799173593521118\n",
      "Loss at step 136 is: 0.7746517062187195\n",
      "Loss at step 137 is: 0.7693643569946289\n",
      "Loss at step 138 is: 0.7640584111213684\n",
      "Loss at step 139 is: 0.7587360739707947\n",
      "Loss at step 140 is: 0.7534015774726868\n",
      "Loss at step 141 is: 0.7480571269989014\n",
      "Loss at step 142 is: 0.7427061796188354\n",
      "Loss at step 143 is: 0.737352192401886\n",
      "Loss at step 144 is: 0.7319973111152649\n",
      "Loss at step 145 is: 0.726645290851593\n",
      "Loss at step 146 is: 0.7212998270988464\n",
      "Loss at step 147 is: 0.7159631848335266\n",
      "Loss at step 148 is: 0.7106387615203857\n",
      "Loss at step 149 is: 0.7053292393684387\n",
      "Loss at step 150 is: 0.7000381946563721\n",
      "Loss at step 151 is: 0.6947683095932007\n",
      "Loss at step 152 is: 0.6895223259925842\n",
      "Loss at step 153 is: 0.6843031644821167\n",
      "Loss at step 154 is: 0.679113507270813\n",
      "Loss at step 155 is: 0.6739563345909119\n",
      "Loss at step 156 is: 0.6688334941864014\n",
      "Loss at step 157 is: 0.6637481451034546\n",
      "Loss at step 158 is: 0.6587017774581909\n",
      "Loss at step 159 is: 0.6536974310874939\n",
      "Loss at step 160 is: 0.6487370133399963\n",
      "Loss at step 161 is: 0.6438222527503967\n",
      "Loss at step 162 is: 0.638955295085907\n",
      "Loss at step 163 is: 0.6341375708580017\n",
      "Loss at step 164 is: 0.6293713450431824\n",
      "Loss at step 165 is: 0.6246574521064758\n",
      "Loss at step 166 is: 0.6199976801872253\n",
      "Loss at step 167 is: 0.6153930425643921\n",
      "Loss at step 168 is: 0.610844612121582\n",
      "Loss at step 169 is: 0.606353759765625\n",
      "Loss at step 170 is: 0.6019216179847717\n",
      "Loss at step 171 is: 0.5975480675697327\n",
      "Loss at step 172 is: 0.5932344198226929\n",
      "Loss at step 173 is: 0.5889815092086792\n",
      "Loss at step 174 is: 0.5847890973091125\n",
      "Loss at step 175 is: 0.5806578397750854\n",
      "Loss at step 176 is: 0.5765876770019531\n",
      "Loss at step 177 is: 0.5725792050361633\n",
      "Loss at step 178 is: 0.5686324238777161\n",
      "Loss at step 179 is: 0.5647470355033875\n",
      "Loss at step 180 is: 0.5609228014945984\n",
      "Loss at step 181 is: 0.5571598410606384\n",
      "Loss at step 182 is: 0.5534579157829285\n",
      "Loss at step 183 is: 0.5498164296150208\n",
      "Loss at step 184 is: 0.5462349057197571\n",
      "Loss at step 185 is: 0.5427128672599792\n",
      "Loss at step 186 is: 0.5392500162124634\n",
      "Loss at step 187 is: 0.5358454585075378\n",
      "Loss at step 188 is: 0.5324988961219788\n",
      "Loss at step 189 is: 0.5292091965675354\n",
      "Loss at step 190 is: 0.52597576379776\n",
      "Loss at step 191 is: 0.5227980017662048\n",
      "Loss at step 192 is: 0.5196747183799744\n",
      "Loss at step 193 is: 0.5166053175926208\n",
      "Loss at step 194 is: 0.5135891437530518\n",
      "Loss at step 195 is: 0.5106247663497925\n",
      "Loss at step 196 is: 0.50771164894104\n",
      "Loss at step 197 is: 0.5048487782478333\n",
      "Loss at step 198 is: 0.5020350217819214\n",
      "Loss at step 199 is: 0.49926990270614624\n",
      "Loss at step 200 is: 0.4965516924858093\n",
      "Loss at step 201 is: 0.4938800632953644\n",
      "Loss at step 202 is: 0.4912538230419159\n",
      "Loss at step 203 is: 0.48867177963256836\n",
      "Loss at step 204 is: 0.4861331582069397\n",
      "Loss at step 205 is: 0.48363688588142395\n",
      "Loss at step 206 is: 0.4811824560165405\n",
      "Loss at step 207 is: 0.47876816987991333\n",
      "Loss at step 208 is: 0.4763932228088379\n",
      "Loss at step 209 is: 0.4740568995475769\n",
      "Loss at step 210 is: 0.47175803780555725\n",
      "Loss at step 211 is: 0.4694961607456207\n",
      "Loss at step 212 is: 0.4672698378562927\n",
      "Loss at step 213 is: 0.4650779962539673\n",
      "Loss at step 214 is: 0.46292033791542053\n",
      "Loss at step 215 is: 0.46079567074775696\n",
      "Loss at step 216 is: 0.45870304107666016\n",
      "Loss at step 217 is: 0.4566415250301361\n",
      "Loss at step 218 is: 0.4546104669570923\n",
      "Loss at step 219 is: 0.4526090919971466\n",
      "Loss at step 220 is: 0.4506363570690155\n",
      "Loss at step 221 is: 0.44869163632392883\n",
      "Loss at step 222 is: 0.44677409529685974\n",
      "Loss at step 223 is: 0.44488292932510376\n",
      "Loss at step 224 is: 0.4430171847343445\n",
      "Loss at step 225 is: 0.44117680191993713\n",
      "Loss at step 226 is: 0.4393604099750519\n",
      "Loss at step 227 is: 0.43756744265556335\n",
      "Loss at step 228 is: 0.4357973039150238\n",
      "Loss at step 229 is: 0.43404945731163025\n",
      "Loss at step 230 is: 0.43232303857803345\n",
      "Loss at step 231 is: 0.4306173324584961\n",
      "Loss at step 232 is: 0.4289320409297943\n",
      "Loss at step 233 is: 0.4272664189338684\n",
      "Loss at step 234 is: 0.42561963200569153\n",
      "Loss at step 235 is: 0.4239914119243622\n",
      "Loss at step 236 is: 0.42238131165504456\n",
      "Loss at step 237 is: 0.42078858613967896\n",
      "Loss at step 238 is: 0.41921260952949524\n",
      "Loss at step 239 is: 0.41765299439430237\n",
      "Loss at step 240 is: 0.41610923409461975\n",
      "Loss at step 241 is: 0.4145807921886444\n",
      "Loss at step 242 is: 0.4130672514438629\n",
      "Loss at step 243 is: 0.41156816482543945\n",
      "Loss at step 244 is: 0.41008302569389343\n",
      "Loss at step 245 is: 0.4086116850376129\n",
      "Loss at step 246 is: 0.4071531891822815\n",
      "Loss at step 247 is: 0.40570762753486633\n",
      "Loss at step 248 is: 0.4042742848396301\n",
      "Loss at step 249 is: 0.4028530716896057\n",
      "Loss at step 250 is: 0.40144336223602295\n",
      "Loss at step 251 is: 0.40004512667655945\n",
      "Loss at step 252 is: 0.3986574709415436\n",
      "Loss at step 253 is: 0.3972804844379425\n",
      "Loss at step 254 is: 0.395913690328598\n",
      "Loss at step 255 is: 0.3945569097995758\n",
      "Loss at step 256 is: 0.39320966601371765\n",
      "Loss at step 257 is: 0.39187172055244446\n",
      "Loss at step 258 is: 0.3905426859855652\n",
      "Loss at step 259 is: 0.38922253251075745\n",
      "Loss at step 260 is: 0.3879108130931854\n",
      "Loss at step 261 is: 0.3866073191165924\n",
      "Loss at step 262 is: 0.38531196117401123\n",
      "Loss at step 263 is: 0.3840242028236389\n",
      "Loss at step 264 is: 0.38274383544921875\n",
      "Loss at step 265 is: 0.3814708888530731\n",
      "Loss at step 266 is: 0.380204975605011\n",
      "Loss at step 267 is: 0.3789457678794861\n",
      "Loss at step 268 is: 0.3776931166648865\n",
      "Loss at step 269 is: 0.3764471411705017\n",
      "Loss at step 270 is: 0.3752073347568512\n",
      "Loss at step 271 is: 0.37397342920303345\n",
      "Loss at step 272 is: 0.3727456033229828\n",
      "Loss at step 273 is: 0.371523380279541\n",
      "Loss at step 274 is: 0.3703067898750305\n",
      "Loss at step 275 is: 0.36909550428390503\n",
      "Loss at step 276 is: 0.367889404296875\n",
      "Loss at step 277 is: 0.36668846011161804\n",
      "Loss at step 278 is: 0.3654926121234894\n",
      "Loss at step 279 is: 0.3643013834953308\n",
      "Loss at step 280 is: 0.36311498284339905\n",
      "Loss at step 281 is: 0.36193305253982544\n",
      "Loss at step 282 is: 0.36075565218925476\n",
      "Loss at step 283 is: 0.3595825135707855\n",
      "Loss at step 284 is: 0.3584136366844177\n",
      "Loss at step 285 is: 0.35724887251853943\n",
      "Loss at step 286 is: 0.35608813166618347\n",
      "Loss at step 287 is: 0.3549313247203827\n",
      "Loss at step 288 is: 0.35377830266952515\n",
      "Loss at step 289 is: 0.3526289761066437\n",
      "Loss at step 290 is: 0.35148337483406067\n",
      "Loss at step 291 is: 0.3503413796424866\n",
      "Loss at step 292 is: 0.3492026925086975\n",
      "Loss at step 293 is: 0.34806764125823975\n",
      "Loss at step 294 is: 0.34693604707717896\n",
      "Loss at step 295 is: 0.34580737352371216\n",
      "Loss at step 296 is: 0.3446822464466095\n",
      "Loss at step 297 is: 0.34356024861335754\n",
      "Loss at step 298 is: 0.34244126081466675\n",
      "Loss at step 299 is: 0.3413255512714386\n",
      "Loss at step 300 is: 0.34021273255348206\n",
      "Loss at step 301 is: 0.3391028642654419\n",
      "Loss at step 302 is: 0.3379959464073181\n",
      "Loss at step 303 is: 0.3368918299674988\n",
      "Loss at step 304 is: 0.33579063415527344\n",
      "Loss at step 305 is: 0.3346923589706421\n",
      "Loss at step 306 is: 0.33359667658805847\n",
      "Loss at step 307 is: 0.33250391483306885\n",
      "Loss at step 308 is: 0.33141377568244934\n",
      "Loss at step 309 is: 0.3303264081478119\n",
      "Loss at step 310 is: 0.32924172282218933\n",
      "Loss at step 311 is: 0.3281596302986145\n",
      "Loss at step 312 is: 0.32708021998405457\n",
      "Loss at step 313 is: 0.32600340247154236\n",
      "Loss at step 314 is: 0.3249291479587555\n",
      "Loss at step 315 is: 0.32385751605033875\n",
      "Loss at step 316 is: 0.3227885365486145\n",
      "Loss at step 317 is: 0.3217219114303589\n",
      "Loss at step 318 is: 0.320657879114151\n",
      "Loss at step 319 is: 0.31959643959999084\n",
      "Loss at step 320 is: 0.3185376226902008\n",
      "Loss at step 321 is: 0.3174811899662018\n",
      "Loss at step 322 is: 0.3164275288581848\n",
      "Loss at step 323 is: 0.3153762221336365\n",
      "Loss at step 324 is: 0.3143273591995239\n",
      "Loss at step 325 is: 0.31328102946281433\n",
      "Loss at step 326 is: 0.31223738193511963\n",
      "Loss at step 327 is: 0.3111962378025055\n",
      "Loss at step 328 is: 0.3101576864719391\n",
      "Loss at step 329 is: 0.3091214895248413\n",
      "Loss at step 330 is: 0.30808785557746887\n",
      "Loss at step 331 is: 0.30705687403678894\n",
      "Loss at step 332 is: 0.3060283362865448\n",
      "Loss at step 333 is: 0.3050024211406708\n",
      "Loss at step 334 is: 0.30397912859916687\n",
      "Loss at step 335 is: 0.30295830965042114\n",
      "Loss at step 336 is: 0.301939994096756\n",
      "Loss at step 337 is: 0.3009245693683624\n",
      "Loss at step 338 is: 0.2999114990234375\n",
      "Loss at step 339 is: 0.29890117049217224\n",
      "Loss at step 340 is: 0.2978934347629547\n",
      "Loss at step 341 is: 0.2968883216381073\n",
      "Loss at step 342 is: 0.29588592052459717\n",
      "Loss at step 343 is: 0.29488614201545715\n",
      "Loss at step 344 is: 0.29388904571533203\n",
      "Loss at step 345 is: 0.29289472103118896\n",
      "Loss at step 346 is: 0.291903018951416\n",
      "Loss at step 347 is: 0.29091402888298035\n",
      "Loss at step 348 is: 0.28992781043052673\n",
      "Loss at step 349 is: 0.2889443039894104\n",
      "Loss at step 350 is: 0.2879636585712433\n",
      "Loss at step 351 is: 0.28698575496673584\n",
      "Loss at step 352 is: 0.2860105633735657\n",
      "Loss at step 353 is: 0.2850382924079895\n",
      "Loss at step 354 is: 0.284068763256073\n",
      "Loss at step 355 is: 0.2831021845340729\n",
      "Loss at step 356 is: 0.2821384370326996\n",
      "Loss at step 357 is: 0.28117746114730835\n",
      "Loss at step 358 is: 0.28021958470344543\n",
      "Loss at step 359 is: 0.2792643904685974\n",
      "Loss at step 360 is: 0.27831223607063293\n",
      "Loss at step 361 is: 0.27736300230026245\n",
      "Loss at step 362 is: 0.2764166295528412\n",
      "Loss at step 363 is: 0.27547332644462585\n",
      "Loss at step 364 is: 0.27453291416168213\n",
      "Loss at step 365 is: 0.2735956907272339\n",
      "Loss at step 366 is: 0.2726612687110901\n",
      "Loss at step 367 is: 0.2717299163341522\n",
      "Loss at step 368 is: 0.2708016335964203\n",
      "Loss at step 369 is: 0.26987650990486145\n",
      "Loss at step 370 is: 0.26895424723625183\n",
      "Loss at step 371 is: 0.26803523302078247\n",
      "Loss at step 372 is: 0.26711928844451904\n",
      "Loss at step 373 is: 0.2662063539028168\n",
      "Loss at step 374 is: 0.26529666781425476\n",
      "Loss at step 375 is: 0.2643900215625763\n",
      "Loss at step 376 is: 0.2634865939617157\n",
      "Loss at step 377 is: 0.2625863254070282\n",
      "Loss at step 378 is: 0.26168912649154663\n",
      "Loss at step 379 is: 0.2607952356338501\n",
      "Loss at step 380 is: 0.2599044442176819\n",
      "Loss at step 381 is: 0.2590169310569763\n",
      "Loss at step 382 is: 0.258132666349411\n",
      "Loss at step 383 is: 0.2572515904903412\n",
      "Loss at step 384 is: 0.25637370347976685\n",
      "Loss at step 385 is: 0.2554991841316223\n",
      "Loss at step 386 is: 0.2546277642250061\n",
      "Loss at step 387 is: 0.25375980138778687\n",
      "Loss at step 388 is: 0.2528950273990631\n",
      "Loss at step 389 is: 0.252033531665802\n",
      "Loss at step 390 is: 0.2511753737926483\n",
      "Loss at step 391 is: 0.2503204643726349\n",
      "Loss at step 392 is: 0.24946899712085724\n",
      "Loss at step 393 is: 0.24862061440944672\n",
      "Loss at step 394 is: 0.2477758228778839\n",
      "Loss at step 395 is: 0.24693430960178375\n",
      "Loss at step 396 is: 0.24609607458114624\n",
      "Loss at step 397 is: 0.24526114761829376\n",
      "Loss at step 398 is: 0.2444297820329666\n",
      "Loss at step 399 is: 0.24360157549381256\n",
      "Loss at step 400 is: 0.2427767664194107\n",
      "Loss at step 401 is: 0.24195538461208344\n",
      "Loss at step 402 is: 0.24113735556602478\n",
      "Loss at step 403 is: 0.24032281339168549\n",
      "Loss at step 404 is: 0.23951157927513123\n",
      "Loss at step 405 is: 0.23870372772216797\n",
      "Loss at step 406 is: 0.23789934813976288\n",
      "Loss at step 407 is: 0.23709829151630402\n",
      "Loss at step 408 is: 0.23630070686340332\n",
      "Loss at step 409 is: 0.23550643026828766\n",
      "Loss at step 410 is: 0.23471562564373016\n",
      "Loss at step 411 is: 0.23392821848392487\n",
      "Loss at step 412 is: 0.23314420878887177\n",
      "Loss at step 413 is: 0.2323637455701828\n",
      "Loss at step 414 is: 0.2315864861011505\n",
      "Loss at step 415 is: 0.23081283271312714\n",
      "Loss at step 416 is: 0.23004256188869476\n",
      "Loss at step 417 is: 0.22927561402320862\n",
      "Loss at step 418 is: 0.22851213812828064\n",
      "Loss at step 419 is: 0.22775207459926605\n",
      "Loss at step 420 is: 0.22699542343616486\n",
      "Loss at step 421 is: 0.22624224424362183\n",
      "Loss at step 422 is: 0.22549237310886383\n",
      "Loss at step 423 is: 0.224745973944664\n",
      "Loss at step 424 is: 0.22400297224521637\n",
      "Loss at step 425 is: 0.22326335310935974\n",
      "Loss at step 426 is: 0.22252725064754486\n",
      "Loss at step 427 is: 0.22179441154003143\n",
      "Loss at step 428 is: 0.22106513381004333\n",
      "Loss at step 429 is: 0.2203390747308731\n",
      "Loss at step 430 is: 0.21961647272109985\n",
      "Loss at step 431 is: 0.21889732778072357\n",
      "Loss at step 432 is: 0.2181815654039383\n",
      "Loss at step 433 is: 0.21746914088726044\n",
      "Loss at step 434 is: 0.21676011383533478\n",
      "Loss at step 435 is: 0.21605433523654938\n",
      "Loss at step 436 is: 0.2153521627187729\n",
      "Loss at step 437 is: 0.21465317904949188\n",
      "Loss at step 438 is: 0.21395757794380188\n",
      "Loss at step 439 is: 0.21326537430286407\n",
      "Loss at step 440 is: 0.2125764787197113\n",
      "Loss at step 441 is: 0.21189098060131073\n",
      "Loss at step 442 is: 0.2112087458372116\n",
      "Loss at step 443 is: 0.2105298787355423\n",
      "Loss at step 444 is: 0.209854394197464\n",
      "Loss at step 445 is: 0.20918206870555878\n",
      "Loss at step 446 is: 0.2085130661725998\n",
      "Loss at step 447 is: 0.2078474462032318\n",
      "Loss at step 448 is: 0.2071850746870041\n",
      "Loss at step 449 is: 0.20652607083320618\n",
      "Loss at step 450 is: 0.20587025582790375\n",
      "Loss at step 451 is: 0.20521779358386993\n",
      "Loss at step 452 is: 0.2045685201883316\n",
      "Loss at step 453 is: 0.20392251014709473\n",
      "Loss at step 454 is: 0.2032797485589981\n",
      "Loss at step 455 is: 0.20264014601707458\n",
      "Loss at step 456 is: 0.2020038664340973\n",
      "Loss at step 457 is: 0.20137068629264832\n",
      "Loss at step 458 is: 0.20074081420898438\n",
      "Loss at step 459 is: 0.20011408627033234\n",
      "Loss at step 460 is: 0.19949057698249817\n",
      "Loss at step 461 is: 0.1988702118396759\n",
      "Loss at step 462 is: 0.19825302064418793\n",
      "Loss at step 463 is: 0.19763900339603424\n",
      "Loss at step 464 is: 0.19702810049057007\n",
      "Loss at step 465 is: 0.19642041623592377\n",
      "Loss at step 466 is: 0.1958158165216446\n",
      "Loss at step 467 is: 0.19521424174308777\n",
      "Loss at step 468 is: 0.19461582601070404\n",
      "Loss at step 469 is: 0.19402052462100983\n",
      "Loss at step 470 is: 0.19342830777168274\n",
      "Loss at step 471 is: 0.19283919036388397\n",
      "Loss at step 472 is: 0.19225306808948517\n",
      "Loss at step 473 is: 0.19166995584964752\n",
      "Loss at step 474 is: 0.19108986854553223\n",
      "Loss at step 475 is: 0.19051297008991241\n",
      "Loss at step 476 is: 0.18993888795375824\n",
      "Loss at step 477 is: 0.18936799466609955\n",
      "Loss at step 478 is: 0.1887999176979065\n",
      "Loss at step 479 is: 0.18823489546775818\n",
      "Loss at step 480 is: 0.18767282366752625\n",
      "Loss at step 481 is: 0.18711380660533905\n",
      "Loss at step 482 is: 0.1865575760602951\n",
      "Loss at step 483 is: 0.1860044002532959\n",
      "Loss at step 484 is: 0.1854541152715683\n",
      "Loss at step 485 is: 0.18490664660930634\n",
      "Loss at step 486 is: 0.18436212837696075\n",
      "Loss at step 487 is: 0.18382053077220917\n",
      "Loss at step 488 is: 0.18328166007995605\n",
      "Loss at step 489 is: 0.1827458292245865\n",
      "Loss at step 490 is: 0.182212695479393\n",
      "Loss at step 491 is: 0.18168237805366516\n",
      "Loss at step 492 is: 0.1811550259590149\n",
      "Loss at step 493 is: 0.18063035607337952\n",
      "Loss at step 494 is: 0.18010848760604858\n",
      "Loss at step 495 is: 0.17958949506282806\n",
      "Loss at step 496 is: 0.17907316982746124\n",
      "Loss at step 497 is: 0.1785595864057541\n",
      "Loss at step 498 is: 0.17804880440235138\n",
      "Loss at step 499 is: 0.17754067480564117\n",
      "Loss at step 500 is: 0.1770353764295578\n",
      "Loss at step 501 is: 0.17653273046016693\n",
      "Loss at step 502 is: 0.17603270709514618\n",
      "Loss at step 503 is: 0.1755354255437851\n",
      "Loss at step 504 is: 0.17504073679447174\n",
      "Loss at step 505 is: 0.1745487004518509\n",
      "Loss at step 506 is: 0.17405937612056732\n",
      "Loss at step 507 is: 0.1735726296901703\n",
      "Loss at step 508 is: 0.17308853566646576\n",
      "Loss at step 509 is: 0.1726069152355194\n",
      "Loss at step 510 is: 0.17212797701358795\n",
      "Loss at step 511 is: 0.171651691198349\n",
      "Loss at step 512 is: 0.17117777466773987\n",
      "Loss at step 513 is: 0.17070649564266205\n",
      "Loss at step 514 is: 0.17023776471614838\n",
      "Loss at step 515 is: 0.1697714924812317\n",
      "Loss at step 516 is: 0.16930776834487915\n",
      "Loss at step 517 is: 0.16884645819664001\n",
      "Loss at step 518 is: 0.1683877408504486\n",
      "Loss at step 519 is: 0.1679314225912094\n",
      "Loss at step 520 is: 0.16747762262821198\n",
      "Loss at step 521 is: 0.16702620685100555\n",
      "Loss at step 522 is: 0.16657716035842896\n",
      "Loss at step 523 is: 0.16613052785396576\n",
      "Loss at step 524 is: 0.16568633913993835\n",
      "Loss at step 525 is: 0.16524457931518555\n",
      "Loss at step 526 is: 0.16480514407157898\n",
      "Loss at step 527 is: 0.16436809301376343\n",
      "Loss at step 528 is: 0.1639333814382553\n",
      "Loss at step 529 is: 0.16350097954273224\n",
      "Loss at step 530 is: 0.1630709320306778\n",
      "Loss at step 531 is: 0.16264314949512482\n",
      "Loss at step 532 is: 0.16221769154071808\n",
      "Loss at step 533 is: 0.16179458796977997\n",
      "Loss at step 534 is: 0.16137361526489258\n",
      "Loss at step 535 is: 0.1609550416469574\n",
      "Loss at step 536 is: 0.16053856909275055\n",
      "Loss at step 537 is: 0.16012436151504517\n",
      "Loss at step 538 is: 0.15971247851848602\n",
      "Loss at step 539 is: 0.1593026965856552\n",
      "Loss at step 540 is: 0.15889516472816467\n",
      "Loss at step 541 is: 0.15848983824253082\n",
      "Loss at step 542 is: 0.15808649361133575\n",
      "Loss at step 543 is: 0.1576855182647705\n",
      "Loss at step 544 is: 0.15728655457496643\n",
      "Loss at step 545 is: 0.15688981115818024\n",
      "Loss at step 546 is: 0.15649515390396118\n",
      "Loss at step 547 is: 0.15610264241695404\n",
      "Loss at step 548 is: 0.1557120978832245\n",
      "Loss at step 549 is: 0.15532377362251282\n",
      "Loss at step 550 is: 0.15493746101856232\n",
      "Loss at step 551 is: 0.15455321967601776\n",
      "Loss at step 552 is: 0.15417100489139557\n",
      "Loss at step 553 is: 0.1537908911705017\n",
      "Loss at step 554 is: 0.15341275930404663\n",
      "Loss at step 555 is: 0.15303656458854675\n",
      "Loss at step 556 is: 0.152662456035614\n",
      "Loss at step 557 is: 0.15229035913944244\n",
      "Loss at step 558 is: 0.1519201695919037\n",
      "Loss at step 559 is: 0.15155191719532013\n",
      "Loss at step 560 is: 0.15118566155433655\n",
      "Loss at step 561 is: 0.15082140266895294\n",
      "Loss at step 562 is: 0.15045899152755737\n",
      "Loss at step 563 is: 0.1500985026359558\n",
      "Loss at step 564 is: 0.14973989129066467\n",
      "Loss at step 565 is: 0.14938320219516754\n",
      "Loss at step 566 is: 0.14902850985527039\n",
      "Loss at step 567 is: 0.14867551624774933\n",
      "Loss at step 568 is: 0.14832450449466705\n",
      "Loss at step 569 is: 0.14797523617744446\n",
      "Loss at step 570 is: 0.14762790501117706\n",
      "Loss at step 571 is: 0.14728230237960815\n",
      "Loss at step 572 is: 0.14693856239318848\n",
      "Loss at step 573 is: 0.14659665524959564\n",
      "Loss at step 574 is: 0.1462564766407013\n",
      "Loss at step 575 is: 0.1459181308746338\n",
      "Loss at step 576 is: 0.14558155834674835\n",
      "Loss at step 577 is: 0.1452467292547226\n",
      "Loss at step 578 is: 0.14491362869739532\n",
      "Loss at step 579 is: 0.14458227157592773\n",
      "Loss at step 580 is: 0.14425261318683624\n",
      "Loss at step 581 is: 0.1439247876405716\n",
      "Loss at step 582 is: 0.1435985565185547\n",
      "Loss at step 583 is: 0.14327406883239746\n",
      "Loss at step 584 is: 0.14295125007629395\n",
      "Loss at step 585 is: 0.14263011515140533\n",
      "Loss at step 586 is: 0.14231060445308685\n",
      "Loss at step 587 is: 0.14199280738830566\n",
      "Loss at step 588 is: 0.14167667925357819\n",
      "Loss at step 589 is: 0.14136214554309845\n",
      "Loss at step 590 is: 0.14104916155338287\n",
      "Loss at step 591 is: 0.14073793590068817\n",
      "Loss at step 592 is: 0.14042824506759644\n",
      "Loss at step 593 is: 0.14012010395526886\n",
      "Loss at step 594 is: 0.1398136019706726\n",
      "Loss at step 595 is: 0.1395086795091629\n",
      "Loss at step 596 is: 0.13920529186725616\n",
      "Loss at step 597 is: 0.1389034539461136\n",
      "Loss at step 598 is: 0.13860316574573517\n",
      "Loss at step 599 is: 0.1383044570684433\n",
      "Loss at step 600 is: 0.138007253408432\n",
      "Loss at step 601 is: 0.1377115398645401\n",
      "Loss at step 602 is: 0.13741733133792877\n",
      "Loss at step 603 is: 0.1371246725320816\n",
      "Loss at step 604 is: 0.13683342933654785\n",
      "Loss at step 605 is: 0.13654373586177826\n",
      "Loss at step 606 is: 0.13625550270080566\n",
      "Loss at step 607 is: 0.1359686702489853\n",
      "Loss at step 608 is: 0.1356833279132843\n",
      "Loss at step 609 is: 0.1353994458913803\n",
      "Loss at step 610 is: 0.13511699438095093\n",
      "Loss at step 611 is: 0.13483595848083496\n",
      "Loss at step 612 is: 0.13455627858638763\n",
      "Loss at step 613 is: 0.1342781037092209\n",
      "Loss at step 614 is: 0.1340012401342392\n",
      "Loss at step 615 is: 0.1337258517742157\n",
      "Loss at step 616 is: 0.13345178961753845\n",
      "Loss at step 617 is: 0.13317908346652985\n",
      "Loss at step 618 is: 0.13290774822235107\n",
      "Loss at step 619 is: 0.13263782858848572\n",
      "Loss at step 620 is: 0.13236922025680542\n",
      "Loss at step 621 is: 0.13210196793079376\n",
      "Loss at step 622 is: 0.13183599710464478\n",
      "Loss at step 623 is: 0.13157141208648682\n",
      "Loss at step 624 is: 0.13130804896354675\n",
      "Loss at step 625 is: 0.1310460865497589\n",
      "Loss at step 626 is: 0.13078539073467255\n",
      "Loss at step 627 is: 0.13052596151828766\n",
      "Loss at step 628 is: 0.13026781380176544\n",
      "Loss at step 629 is: 0.13001099228858948\n",
      "Loss at step 630 is: 0.1297553926706314\n",
      "Loss at step 631 is: 0.12950098514556885\n",
      "Loss at step 632 is: 0.12924793362617493\n",
      "Loss at step 633 is: 0.1289960741996765\n",
      "Loss at step 634 is: 0.12874546647071838\n",
      "Loss at step 635 is: 0.12849602103233337\n",
      "Loss at step 636 is: 0.12824785709381104\n",
      "Loss at step 637 is: 0.1280008852481842\n",
      "Loss at step 638 is: 0.12775512039661407\n",
      "Loss at step 639 is: 0.12751062214374542\n",
      "Loss at step 640 is: 0.12726718187332153\n",
      "Loss at step 641 is: 0.12702502310276031\n",
      "Loss at step 642 is: 0.12678401172161102\n",
      "Loss at step 643 is: 0.12654411792755127\n",
      "Loss at step 644 is: 0.1263054609298706\n",
      "Loss at step 645 is: 0.1260678470134735\n",
      "Loss at step 646 is: 0.1258315145969391\n",
      "Loss at step 647 is: 0.12559615075588226\n",
      "Loss at step 648 is: 0.12536205351352692\n",
      "Loss at step 649 is: 0.12512904405593872\n",
      "Loss at step 650 is: 0.12489715218544006\n",
      "Loss at step 651 is: 0.12466634064912796\n",
      "Loss at step 652 is: 0.1244366466999054\n",
      "Loss at step 653 is: 0.12420805543661118\n",
      "Loss at step 654 is: 0.1239805519580841\n",
      "Loss at step 655 is: 0.12375413626432419\n",
      "Loss at step 656 is: 0.12352879345417023\n",
      "Loss at step 657 is: 0.12330453842878342\n",
      "Loss at step 658 is: 0.12308129668235779\n",
      "Loss at step 659 is: 0.12285913527011871\n",
      "Loss at step 660 is: 0.1226380243897438\n",
      "Loss at step 661 is: 0.12241796404123306\n",
      "Loss at step 662 is: 0.12219889461994171\n",
      "Loss at step 663 is: 0.12198089063167572\n",
      "Loss at step 664 is: 0.12176390737295151\n",
      "Loss at step 665 is: 0.12154792994260788\n",
      "Loss at step 666 is: 0.12133298069238663\n",
      "Loss at step 667 is: 0.12111898511648178\n",
      "Loss at step 668 is: 0.12090600281953812\n",
      "Loss at step 669 is: 0.1206941083073616\n",
      "Loss at step 670 is: 0.12048307806253433\n",
      "Loss at step 671 is: 0.12027308344841003\n",
      "Loss at step 672 is: 0.12006407231092453\n",
      "Loss at step 673 is: 0.11985602974891663\n",
      "Loss at step 674 is: 0.11964892596006393\n",
      "Loss at step 675 is: 0.11944276839494705\n",
      "Loss at step 676 is: 0.11923757940530777\n",
      "Loss at step 677 is: 0.11903335154056549\n",
      "Loss at step 678 is: 0.11883004009723663\n",
      "Loss at step 679 is: 0.1186276525259018\n",
      "Loss at step 680 is: 0.11842620372772217\n",
      "Loss at step 681 is: 0.11822565644979477\n",
      "Loss at step 682 is: 0.11802604049444199\n",
      "Loss at step 683 is: 0.11782733350992203\n",
      "Loss at step 684 is: 0.11762956529855728\n",
      "Loss at step 685 is: 0.1174326092004776\n",
      "Loss at step 686 is: 0.11723662167787552\n",
      "Loss at step 687 is: 0.11704146862030029\n",
      "Loss at step 688 is: 0.11684725433588028\n",
      "Loss at step 689 is: 0.11665388941764832\n",
      "Loss at step 690 is: 0.11646139621734619\n",
      "Loss at step 691 is: 0.1162697821855545\n",
      "Loss at step 692 is: 0.11607901006937027\n",
      "Loss at step 693 is: 0.11588910222053528\n",
      "Loss at step 694 is: 0.11570002883672714\n",
      "Loss at step 695 is: 0.11551184952259064\n",
      "Loss at step 696 is: 0.11532444506883621\n",
      "Loss at step 697 is: 0.11513794213533401\n",
      "Loss at step 698 is: 0.11495225131511688\n",
      "Loss at step 699 is: 0.11476731300354004\n",
      "Loss at step 700 is: 0.11458325386047363\n",
      "Loss at step 701 is: 0.11440002173185349\n",
      "Loss at step 702 is: 0.114217609167099\n",
      "Loss at step 703 is: 0.11403591185808182\n",
      "Loss at step 704 is: 0.11385514587163925\n",
      "Loss at step 705 is: 0.11367505043745041\n",
      "Loss at step 706 is: 0.11349581927061081\n",
      "Loss at step 707 is: 0.11331739276647568\n",
      "Loss at step 708 is: 0.11313972622156143\n",
      "Loss at step 709 is: 0.11296281963586807\n",
      "Loss at step 710 is: 0.1127866581082344\n",
      "Loss at step 711 is: 0.1126113310456276\n",
      "Loss at step 712 is: 0.11243674159049988\n",
      "Loss at step 713 is: 0.11226285248994827\n",
      "Loss at step 714 is: 0.11208976805210114\n",
      "Loss at step 715 is: 0.11191748082637787\n",
      "Loss at step 716 is: 0.11174587160348892\n",
      "Loss at step 717 is: 0.11157504469156265\n",
      "Loss at step 718 is: 0.1114048957824707\n",
      "Loss at step 719 is: 0.11123554408550262\n",
      "Loss at step 720 is: 0.11106690764427185\n",
      "Loss at step 721 is: 0.1108989492058754\n",
      "Loss at step 722 is: 0.11073175817728043\n",
      "Loss at step 723 is: 0.11056523770093918\n",
      "Loss at step 724 is: 0.11039949208498001\n",
      "Loss at step 725 is: 0.11023443192243576\n",
      "Loss at step 726 is: 0.11007001250982285\n",
      "Loss at step 727 is: 0.10990633070468903\n",
      "Loss at step 728 is: 0.10974340885877609\n",
      "Loss at step 729 is: 0.10958104580640793\n",
      "Loss at step 730 is: 0.10941945761442184\n",
      "Loss at step 731 is: 0.1092585027217865\n",
      "Loss at step 732 is: 0.10909827798604965\n",
      "Loss at step 733 is: 0.10893871635198593\n",
      "Loss at step 734 is: 0.10877979546785355\n",
      "Loss at step 735 is: 0.1086215004324913\n",
      "Loss at step 736 is: 0.10846397280693054\n",
      "Loss at step 737 is: 0.10830705612897873\n",
      "Loss at step 738 is: 0.10815081745386124\n",
      "Loss at step 739 is: 0.1079951599240303\n",
      "Loss at step 740 is: 0.10784021019935608\n",
      "Loss at step 741 is: 0.10768584161996841\n",
      "Loss at step 742 is: 0.10753221064805984\n",
      "Loss at step 743 is: 0.10737916082143784\n",
      "Loss at step 744 is: 0.10722670704126358\n",
      "Loss at step 745 is: 0.10707497596740723\n",
      "Loss at step 746 is: 0.10692378133535385\n",
      "Loss at step 747 is: 0.10677318274974823\n",
      "Loss at step 748 is: 0.10662327706813812\n",
      "Loss at step 749 is: 0.10647395998239517\n",
      "Loss at step 750 is: 0.10632530599832535\n",
      "Loss at step 751 is: 0.10617721080780029\n",
      "Loss at step 752 is: 0.10602971166372299\n",
      "Loss at step 753 is: 0.10588281601667404\n",
      "Loss at step 754 is: 0.10573650151491165\n",
      "Loss at step 755 is: 0.10559077560901642\n",
      "Loss at step 756 is: 0.10544569045305252\n",
      "Loss at step 757 is: 0.1053011417388916\n",
      "Loss at step 758 is: 0.10515716671943665\n",
      "Loss at step 759 is: 0.10501385480165482\n",
      "Loss at step 760 is: 0.10487105697393417\n",
      "Loss at step 761 is: 0.10472876578569412\n",
      "Loss at step 762 is: 0.10458719730377197\n",
      "Loss at step 763 is: 0.10444612056016922\n",
      "Loss at step 764 is: 0.10430555790662766\n",
      "Loss at step 765 is: 0.10416562855243683\n",
      "Loss at step 766 is: 0.10402622818946838\n",
      "Loss at step 767 is: 0.1038874015212059\n",
      "Loss at step 768 is: 0.1037491112947464\n",
      "Loss at step 769 is: 0.10361132770776749\n",
      "Loss at step 770 is: 0.10347414016723633\n",
      "Loss at step 771 is: 0.10333750396966934\n",
      "Loss at step 772 is: 0.10320135951042175\n",
      "Loss at step 773 is: 0.10306576639413834\n",
      "Loss at step 774 is: 0.1029307171702385\n",
      "Loss at step 775 is: 0.10279615968465805\n",
      "Loss at step 776 is: 0.10266219824552536\n",
      "Loss at step 777 is: 0.10252871364355087\n",
      "Loss at step 778 is: 0.10239573568105698\n",
      "Loss at step 779 is: 0.10226331651210785\n",
      "Loss at step 780 is: 0.10213135927915573\n",
      "Loss at step 781 is: 0.10199996083974838\n",
      "Loss at step 782 is: 0.10186906903982162\n",
      "Loss at step 783 is: 0.10173864662647247\n",
      "Loss at step 784 is: 0.10160882025957108\n",
      "Loss at step 785 is: 0.10147940367460251\n",
      "Loss at step 786 is: 0.10135052353143692\n",
      "Loss at step 787 is: 0.10122210532426834\n",
      "Loss at step 788 is: 0.10109415650367737\n",
      "Loss at step 789 is: 0.10096673667430878\n",
      "Loss at step 790 is: 0.10083983838558197\n",
      "Loss at step 791 is: 0.10071335732936859\n",
      "Loss at step 792 is: 0.10058743506669998\n",
      "Loss at step 793 is: 0.10046198964118958\n",
      "Loss at step 794 is: 0.10033692419528961\n",
      "Loss at step 795 is: 0.10021238774061203\n",
      "Loss at step 796 is: 0.10008830577135086\n",
      "Loss at step 797 is: 0.09996474534273148\n",
      "Loss at step 798 is: 0.09984162449836731\n",
      "Loss at step 799 is: 0.09971894323825836\n",
      "Loss at step 800 is: 0.09959671646356583\n",
      "Loss at step 801 is: 0.09947501868009567\n",
      "Loss at step 802 is: 0.09935370087623596\n",
      "Loss at step 803 is: 0.09923288971185684\n",
      "Loss at step 804 is: 0.09911250323057175\n",
      "Loss at step 805 is: 0.09899253398180008\n",
      "Loss at step 806 is: 0.09887305647134781\n",
      "Loss at step 807 is: 0.09875401109457016\n",
      "Loss at step 808 is: 0.09863542765378952\n",
      "Loss at step 809 is: 0.09851723164319992\n",
      "Loss at step 810 is: 0.09839951246976852\n",
      "Loss at step 811 is: 0.09828220307826996\n",
      "Loss at step 812 is: 0.0981653481721878\n",
      "Loss at step 813 is: 0.09804888814687729\n",
      "Loss at step 814 is: 0.09793288260698318\n",
      "Loss at step 815 is: 0.0978173315525055\n",
      "Loss at step 816 is: 0.09770219027996063\n",
      "Loss at step 817 is: 0.09758742898702621\n",
      "Loss at step 818 is: 0.09747307747602463\n",
      "Loss at step 819 is: 0.09735924750566483\n",
      "Loss at step 820 is: 0.0972457304596901\n",
      "Loss at step 821 is: 0.09713266044855118\n",
      "Loss at step 822 is: 0.0970199778676033\n",
      "Loss at step 823 is: 0.09690771996974945\n",
      "Loss at step 824 is: 0.09679587930440903\n",
      "Loss at step 825 is: 0.09668442606925964\n",
      "Loss at step 826 is: 0.0965733528137207\n",
      "Loss at step 827 is: 0.09646271914243698\n",
      "Loss at step 828 is: 0.09635245054960251\n",
      "Loss at step 829 is: 0.09624258428812027\n",
      "Loss at step 830 is: 0.09613312780857086\n",
      "Loss at step 831 is: 0.0960240438580513\n",
      "Loss at step 832 is: 0.09591538459062576\n",
      "Loss at step 833 is: 0.09580706059932709\n",
      "Loss at step 834 is: 0.09569916874170303\n",
      "Loss at step 835 is: 0.09559159725904465\n",
      "Loss at step 836 is: 0.0954844132065773\n",
      "Loss at step 837 is: 0.09537766128778458\n",
      "Loss at step 838 is: 0.09527132660150528\n",
      "Loss at step 839 is: 0.09516524523496628\n",
      "Loss at step 840 is: 0.09505961835384369\n",
      "Loss at step 841 is: 0.09495434165000916\n",
      "Loss at step 842 is: 0.09484944492578506\n",
      "Loss at step 843 is: 0.09474487602710724\n",
      "Loss at step 844 is: 0.09464067965745926\n",
      "Loss at step 845 is: 0.09453686326742172\n",
      "Loss at step 846 is: 0.09443341195583344\n",
      "Loss at step 847 is: 0.0943303033709526\n",
      "Loss at step 848 is: 0.09422758966684341\n",
      "Loss at step 849 is: 0.0941251814365387\n",
      "Loss at step 850 is: 0.09402313828468323\n",
      "Loss at step 851 is: 0.0939214676618576\n",
      "Loss at step 852 is: 0.09382014721632004\n",
      "Loss at step 853 is: 0.09371917694807053\n",
      "Loss at step 854 is: 0.09361851215362549\n",
      "Loss at step 855 is: 0.09351827949285507\n",
      "Loss at step 856 is: 0.09341829270124435\n",
      "Loss at step 857 is: 0.09331874549388885\n",
      "Loss at step 858 is: 0.09321951866149902\n",
      "Loss at step 859 is: 0.09312053769826889\n",
      "Loss at step 860 is: 0.093021921813488\n",
      "Loss at step 861 is: 0.09292369335889816\n",
      "Loss at step 862 is: 0.092825748026371\n",
      "Loss at step 863 is: 0.09272818267345428\n",
      "Loss at step 864 is: 0.09263094514608383\n",
      "Loss at step 865 is: 0.09253402054309845\n",
      "Loss at step 866 is: 0.09243737906217575\n",
      "Loss at step 867 is: 0.09234108030796051\n",
      "Loss at step 868 is: 0.09224516898393631\n",
      "Loss at step 869 is: 0.0921495109796524\n",
      "Loss at step 870 is: 0.09205417335033417\n",
      "Loss at step 871 is: 0.09195921570062637\n",
      "Loss at step 872 is: 0.09186451882123947\n",
      "Loss at step 873 is: 0.09177014976739883\n",
      "Loss at step 874 is: 0.09167610108852386\n",
      "Loss at step 875 is: 0.09158236533403397\n",
      "Loss at step 876 is: 0.09148892015218735\n",
      "Loss at step 877 is: 0.09139582514762878\n",
      "Loss at step 878 is: 0.09130296856164932\n",
      "Loss at step 879 is: 0.0912104919552803\n",
      "Loss at step 880 is: 0.09111826121807098\n",
      "Loss at step 881 is: 0.09102638065814972\n",
      "Loss at step 882 is: 0.09093476086854935\n",
      "Loss at step 883 is: 0.09084347635507584\n",
      "Loss at step 884 is: 0.09075247496366501\n",
      "Loss at step 885 is: 0.09066180884838104\n",
      "Loss at step 886 is: 0.09057137370109558\n",
      "Loss at step 887 is: 0.09048125147819519\n",
      "Loss at step 888 is: 0.09039142727851868\n",
      "Loss at step 889 is: 0.09030187129974365\n",
      "Loss at step 890 is: 0.09021265804767609\n",
      "Loss at step 891 is: 0.0901237353682518\n",
      "Loss at step 892 is: 0.09003504365682602\n",
      "Loss at step 893 is: 0.08994665741920471\n",
      "Loss at step 894 is: 0.08985854685306549\n",
      "Loss at step 895 is: 0.08977074176073074\n",
      "Loss at step 896 is: 0.08968318998813629\n",
      "Loss at step 897 is: 0.08959594368934631\n",
      "Loss at step 898 is: 0.08950898796319962\n",
      "Loss at step 899 is: 0.0894223153591156\n",
      "Loss at step 900 is: 0.0893358513712883\n",
      "Loss at step 901 is: 0.08924970030784607\n",
      "Loss at step 902 is: 0.08916384726762772\n",
      "Loss at step 903 is: 0.08907821774482727\n",
      "Loss at step 904 is: 0.08899294584989548\n",
      "Loss at step 905 is: 0.0889078676700592\n",
      "Loss at step 906 is: 0.08882305771112442\n",
      "Loss at step 907 is: 0.08873851597309113\n",
      "Loss at step 908 is: 0.0886542946100235\n",
      "Loss at step 909 is: 0.08857030421495438\n",
      "Loss at step 910 is: 0.08848657459020615\n",
      "Loss at step 911 is: 0.08840310573577881\n",
      "Loss at step 912 is: 0.08831986039876938\n",
      "Loss at step 913 is: 0.08823689818382263\n",
      "Loss at step 914 is: 0.08815421909093857\n",
      "Loss at step 915 is: 0.0880718007683754\n",
      "Loss at step 916 is: 0.08798959851264954\n",
      "Loss at step 917 is: 0.08790771663188934\n",
      "Loss at step 918 is: 0.08782602846622467\n",
      "Loss at step 919 is: 0.08774459362030029\n",
      "Loss at step 920 is: 0.087663434445858\n",
      "Loss at step 921 is: 0.08758249878883362\n",
      "Loss at step 922 is: 0.08750183880329132\n",
      "Loss at step 923 is: 0.08742143958806992\n",
      "Loss at step 924 is: 0.08734125643968582\n",
      "Loss at step 925 is: 0.08726134151220322\n",
      "Loss at step 926 is: 0.08718161284923553\n",
      "Loss at step 927 is: 0.08710218966007233\n",
      "Loss at step 928 is: 0.08702298253774643\n",
      "Loss at step 929 is: 0.08694401383399963\n",
      "Loss at step 930 is: 0.08686529844999313\n",
      "Loss at step 931 is: 0.08678683638572693\n",
      "Loss at step 932 is: 0.08670854568481445\n",
      "Loss at step 933 is: 0.08663058280944824\n",
      "Loss at step 934 is: 0.08655276894569397\n",
      "Loss at step 935 is: 0.08647524565458298\n",
      "Loss at step 936 is: 0.08639790862798691\n",
      "Loss at step 937 is: 0.08632084727287292\n",
      "Loss at step 938 is: 0.08624399453401566\n",
      "Loss at step 939 is: 0.08616737276315689\n",
      "Loss at step 940 is: 0.08609100431203842\n",
      "Loss at step 941 is: 0.08601488173007965\n",
      "Loss at step 942 is: 0.08593889325857162\n",
      "Loss at step 943 is: 0.08586321026086807\n",
      "Loss at step 944 is: 0.08578772097826004\n",
      "Loss at step 945 is: 0.08571244031190872\n",
      "Loss at step 946 is: 0.08563745021820068\n",
      "Loss at step 947 is: 0.08556260168552399\n",
      "Loss at step 948 is: 0.08548800647258759\n",
      "Loss at step 949 is: 0.0854136273264885\n",
      "Loss at step 950 is: 0.0853395089507103\n",
      "Loss at step 951 is: 0.08526559174060822\n",
      "Loss at step 952 is: 0.08519181609153748\n",
      "Loss at step 953 is: 0.08511832356452942\n",
      "Loss at step 954 is: 0.08504500985145569\n",
      "Loss at step 955 is: 0.08497192710638046\n",
      "Loss at step 956 is: 0.08489906042814255\n",
      "Loss at step 957 is: 0.08482642471790314\n",
      "Loss at step 958 is: 0.08475397527217865\n",
      "Loss at step 959 is: 0.08468175679445267\n",
      "Loss at step 960 is: 0.0846097320318222\n",
      "Loss at step 961 is: 0.08453789353370667\n",
      "Loss at step 962 is: 0.08446632325649261\n",
      "Loss at step 963 is: 0.08439487963914871\n",
      "Loss at step 964 is: 0.0843236893415451\n",
      "Loss at step 965 is: 0.0842527374625206\n",
      "Loss at step 966 is: 0.08418192714452744\n",
      "Loss at step 967 is: 0.08411133289337158\n",
      "Loss at step 968 is: 0.08404093235731125\n",
      "Loss at step 969 is: 0.08397076278924942\n",
      "Loss at step 970 is: 0.08390078693628311\n",
      "Loss at step 971 is: 0.08383099734783173\n",
      "Loss at step 972 is: 0.08376138657331467\n",
      "Loss at step 973 is: 0.08369200676679611\n",
      "Loss at step 974 is: 0.08362279832363129\n",
      "Loss at step 975 is: 0.08355378359556198\n",
      "Loss at step 976 is: 0.08348501473665237\n",
      "Loss at step 977 is: 0.08341638743877411\n",
      "Loss at step 978 is: 0.08334795385599136\n",
      "Loss at step 979 is: 0.08327975869178772\n",
      "Loss at step 980 is: 0.08321170508861542\n",
      "Loss at step 981 is: 0.08314386755228043\n",
      "Loss at step 982 is: 0.08307619392871857\n",
      "Loss at step 983 is: 0.08300872892141342\n",
      "Loss at step 984 is: 0.08294147998094559\n",
      "Loss at step 985 is: 0.08287438005208969\n",
      "Loss at step 986 is: 0.0828074961900711\n",
      "Loss at step 987 is: 0.08274075388908386\n",
      "Loss at step 988 is: 0.08267423510551453\n",
      "Loss at step 989 is: 0.08260788768529892\n",
      "Loss at step 990 is: 0.08254171162843704\n",
      "Loss at step 991 is: 0.08247571438550949\n",
      "Loss at step 992 is: 0.08240989595651627\n",
      "Loss at step 993 is: 0.08234428614377975\n",
      "Loss at step 994 is: 0.08227887004613876\n",
      "Loss at step 995 is: 0.08221360296010971\n",
      "Loss at step 996 is: 0.08214852213859558\n",
      "Loss at step 997 is: 0.08208359777927399\n",
      "Loss at step 998 is: 0.08201885968446732\n",
      "Loss at step 999 is: 0.08195429295301437\n",
      "Finished Training!\n",
      "\n",
      " This model got 96.66666666666667 % right\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    nnet_optimizer.zero_grad()\n",
    "    \n",
    "    # FORWARD-PROPAGATION     \n",
    "    Y_hat = nnet_model(X_train)\n",
    "\n",
    "    loss = nnet_loss_function(Y_hat, Y_train)\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    loss.backward()\n",
    "    print(\"Loss at step\", i, \"is:\" , loss.data.item())\n",
    "    # UPDATE WEIGHTS\n",
    "    nnet_optimizer.step()\n",
    "    \n",
    "print('Finished Training!')\n",
    "    \n",
    "Y_hat_test = nnet_model(X_test)\n",
    "Y_predicted = torch.max(Y_hat_test, 1).indices\n",
    "\n",
    "print(\"\\n This model got\", accuracy_score(Y_test, Y_predicted)*100, \"% right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Wine Classification with PyTorch\n",
    "\n",
    "Now you try. Let's go back to the Wine dataset and train a model with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['wine_type','alcohol', 'malic_acid','ash','alcalinity_of_ash','magnesium',\n",
    "           'total_phenols','flavanoids','nonflavanoid_phenols','proanthocyanins','color_intensity','hue','OD280_OD315','proline']\n",
    "\n",
    "wine_dataset = read_csv(####)\n",
    "\n",
    "# ENCODE SPECIES AS CATEGORY NUMBERS \n",
    "wine_dataset.loc[wine_dataset.wine_type=='wine_1', 'wine_type'] = ###\n",
    "wine_dataset.loc[wine_dataset.wine_type=='wine_2', 'wine_type'] = ###\n",
    "wine_dataset.loc[wine_dataset.wine_type=='wine_3', 'wine_type'] = ###\n",
    "\n",
    "X = wine_dataset.values[###].astype('float32')\n",
    "Y = wine_dataset.values[###].astype('int32')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = ####\n",
    "\n",
    "# CONVERT DATASETS TO PYTORCH TENSORS\n",
    "X_train = Variable(torch.Tensor(X_train).float())\n",
    "X_test = Variable(torch.Tensor(X_test).float())\n",
    "Y_train= Variable(torch.Tensor(Y_train).long())\n",
    "Y_test = Variable(torch.Tensor(Y_test).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# High Performance Machine Learning\n",
    "\n",
    "So far we have trained models using **scikit-learn** and **PyTorch** on small datasets, where any modern Laptop can carry out the work in a matter of seconds. But what do you do when your problem involves massive amounts of data and training complex models many times over until you get results you are happy with?\n",
    "\n",
    "If you recall when we introduced **PyTorch** on the previous notebook, we listed GPU acceleration, tools to handle massive datasets and user-friendly parallel/distributed training as advantages over **scikit-learn**.\n",
    "\n",
    "We will now explore these advantages.\n",
    "\n",
    "## GPU Acceleration\n",
    "\n",
    "When people first learn about how computers work, the CPU is often referred to as the \"brain of the computer\". And in many ways it is. The CPU has to carry out many different types of instructions to enable everything from responding to keyboard/mouse events to playing YouTube videos while you type into a word processor at the same time. In other words, CPUs are built to be good at doing different types of things at the same time.\n",
    "\n",
    "The Graphical Processing Unit (a.k.a the GPU) on the other hand, only has to be good at one thing: rendering graphics. \n",
    "\n",
    "As it turns out, being good at rendering graphics really means being good at performing Vector/Matrix/Tensor operations - the same type of operations that happen under the hood in Neural Networks. It follows that GPUs are better than CPUs at training Neural Networks.\n",
    "\n",
    "Modern Machine Learning libraries, like **PyTorch** allow you to take advantage of this fact and use GPUs to train models faster.\n",
    "\n",
    "Let's see this in action:\n",
    "\n",
    "## Example - Training a Neural Network with GPU:\n",
    "\n",
    "In this example, we will take the Iris Dataset and train a model using the GPU instead of the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE OUR IRIS TRAINING AND TEST SETS AGAIN\n",
    "\n",
    "X = iris_dataset.values[:,0:4].astype('float32')\n",
    "Y = iris_dataset.values[:,4].astype('int32')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "\n",
    "# CONVERT DATASETS TO PYTORCH TENSORS\n",
    "X_train = Variable(torch.Tensor(X_train).float())\n",
    "X_test = Variable(torch.Tensor(X_test).float())\n",
    "Y_train= Variable(torch.Tensor(Y_train).long())\n",
    "Y_test = Variable(torch.Tensor(Y_test).long())\n",
    "\n",
    "\n",
    "# THIS LINE ENABLES GPU ONLY IF A GPU AND CUDA ARE AVAILABLE\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"The device Object is tied to:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE OUR NEURAL NET\n",
    "\n",
    "nnet_model = NeuralNet()\n",
    "\n",
    "# LOAD OUR MODEL ON THE GPU\n",
    "nnet_model.to(device)\n",
    "\n",
    "# LOAD TRAINING DATA ON THE GPU\n",
    "\n",
    "X_train.to(device)\n",
    "Y_train.to(device)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # FORWARD-PROPAGATION     \n",
    "    Y_hat = nnet_model(X_train)\n",
    "\n",
    "    loss = loss_function(Y_hat, Y_train)\n",
    "    #print(\"Loss at step\", i, \"is:\" , loss.data.item())\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    loss.backward()\n",
    "\n",
    "    # UPDATE WEIGHTS\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Massive Amounts of Data - Loaders, Datasets and Batch Training\n",
    "\n",
    "In the example above, our dataset was still very small - we can be pretty sure it will fit entirely inside any modern Graphics Card's memory when we load it on the GPU. But what if we had not a small and nicely structured csv file, but a large-ish dataset of, say... 200GB worth of video and image files? Unless you have the ultimate gaming station, you will probably have a hard time fitting all that in memory whether it's on your Graphics Card or not.\n",
    "\n",
    "With that in mind, **PyTorch** offers a convenient set of tools called *Datasets* and *Loaders* that, when used in conjunction with a Machine Learning technique called *Mini-Batch Training*, offer a good balance between performance, stability and scalability.\n",
    "\n",
    "Let's see how this works using the MNIST Handwritten Digit Dataset from the previous notebook (which is still a very small dataset).\n",
    "\n",
    "### Example - Using Torchvision and Built-in Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms #TORCHVISION IS A SUBPACKAGE THAT CONTAINS MANY INTERESTING UTILITIES FOR HANDLING IMAGES\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# USE TRANSFORMS TO RE-SCALE, NORMALIZE... TRANSFORM! HERE WE JUST GO FROM NUMPY ARRAY TO PYTORCH TENSOR\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_trainingset = datasets.ImageFolder(root='./MNIST-Dataset/Training', transform=transform)\n",
    "\n",
    "# NOTICE BATCH_SIZE AND NUM_WORKERS PARAMETERS - LOAD SMALL A AMOUNT AT A TIME, IN PARALLEL!\n",
    "trainingset_loader = torch.utils.data.DataLoader(mnist_trainingset, batch_size=10,shuffle=True, num_workers=2)\n",
    "\n",
    "# LET'S LOAD A SINGLE BATCH AND DISPLAY IT\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(trainingset_loader)\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(make_grid(images))\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S NOT FORGET TO LOAD THE TEST SET\n",
    "\n",
    "mnist_testset = datasets.ImageFolder(root='./MNIST-Dataset/Test', transform=transform)\n",
    "testset_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHAT SIZE SHOULD OUR NETWORK BE?\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET'S TRAIN A MODEL\n",
    "\n",
    "class ImageLogisticRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ImageLogisticRegression, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(3*28*28, 3)\n",
    "    def forward(self, X):\n",
    "        #FLATTEN THE TENSOR INTO A (3*28*28,1) VECTOR\n",
    "        X = X.reshape(-1,3*28*28)\n",
    "        X = self.fc1(X)\n",
    "\n",
    "        return X  \n",
    "\n",
    "model = ImageLogisticRegression()\n",
    "\n",
    "#model.to(device) # LOAD MODEL ON GPU\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "loss_function = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    for i, data in enumerate(trainingset_loader, 0):\n",
    "        X_batch, Y_batch = data\n",
    "        \n",
    "        X_batch.to(device) # LOAD DATA ON GPU ONE BATCH AT A TIME\n",
    "        Y_batch.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Y_hat = model(X_batch)\n",
    "        loss = loss_function(Y_hat, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(\"Final Loss:\",loss.data.item())\n",
    "print('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used built-in utilities and trained a model on image files. But what if you need to load a massive csv or other types of data for which there is no built-in option available? In those cases you can build your own Custom Datasets and Loaders.\n",
    "\n",
    "### Example - Loading the Iris Dataset CSV file using Custom Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# CUSTOM DATASETS INHERIT FROM PYTORCH CLASS 'Dataset'\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.iris_dataset = pd.read_csv(path, iterator=True, names=['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species'])    \n",
    "\n",
    "    # THIS METHOD DRIVES WHAT HAPPENS WHEN A LOADER IS USED TO LOAD A BATCH\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.iris_dataset.get_chunk(1)\n",
    "        \n",
    "        row.loc[row.species=='Iris-setosa', 'species'] = 0\n",
    "        row.loc[row.species=='Iris-versicolor', 'species'] = 1\n",
    "        row.loc[row.species=='Iris-virginica', 'species'] = 2\n",
    "        \n",
    "        X = row.values[:,0:4].astype('float32')\n",
    "        Y = row.values[:,4].astype('int32')\n",
    "       \n",
    "        X = Variable(torch.Tensor(X).float())\n",
    "        Y = Variable(torch.Tensor(Y).long())\n",
    "    \n",
    "        return X,Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return 150 \n",
    "    \n",
    "iris_dataset = CSVDataset('./data/iris.csv')\n",
    "iris_loader = torch.utils.data.DataLoader(iris_dataset, batch_size=10,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "loss_function = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "for epoch in range(3): \n",
    "    for i, data in enumerate(iris_loader, 0):\n",
    "        X_batch, Y_batch = data\n",
    "        \n",
    "        X_batch = X_batch.reshape(-1,4) #CHOPPING ONE DIMENSION OFF OF LOADER OUTPUT\n",
    "        Y_batch = Y_batch.reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Y_hat = model(X_batch)\n",
    "        loss = loss_function(Y_hat, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(\"Final Loss:\",loss.data.item())        \n",
    "print('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Datasets, Loaders, Mini-Batching and GPU\n",
    "\n",
    "Now you try. Load either the Fruit Images dataset or the Wine csv and train a model using mini-batching and GPU. If you're looking for a challenging, try implementing a Custom Dataset for the Fruit Images dataset instead of using the built-in utility from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Multiple GPUs\n",
    "\n",
    "The last advantage of **PyTorch** over **sckit-learn** on our list of performance enhancements was user-friendly parallel/distributed training. You have seen how to enable training a entire model on a single GPU so far, but what if you have multiple GPUs at your disposal as it is the case on **Compute Canada**'s clusters? Can this accelerate training even more? \n",
    "\n",
    "You bet.\n",
    "\n",
    "As you will see in the next example, doing this in PyTorch is as simple as adding a couple extra lines of code to the single GPU case. Concretely, what this will do is what we call \"Data Parallelism\". Very literally, PyTorch will split your inputs into a number of parts equal to however many GPUs are available and run your training loop on them in parallel. This works very well in simple cases like the kinds of neural networks we've been training so far, but extra care need to be taken if your models are more complex than just a series of torch.nn modules.\n",
    "\n",
    "There is another type of Parallelism that is possible on PyTorch, but that we will not cover in this introductory workshop: multi-node distributed training. In this type of parallelism, you may have not only multiple GPUs, but multiple computers, each maybe with its own set of multiple GPUs.\n",
    "\n",
    "## Example - Training on Multiple GPUs\n",
    "\n",
    "In this example, we'll revisit the MNIST handwritten digit dataset and train a model using multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#LOAD THE SAME MODEL ARCHITECTURE FROM THE SINGLE GPU EXAMPLE\n",
    "model = ImageLogisticRegression()\n",
    "\n",
    "# USE nn.DataParallel TO WRAP THE MODEL\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "# THIS TIME SEND MODEL TO MULTIPLE GPUs    \n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "loss_function = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2): \n",
    "\n",
    "    for i, data in enumerate(trainingset_loader, 0):\n",
    "        X_batch, Y_batch = data\n",
    "        \n",
    "        X_batch.to(device) # LOAD DATA ON GPU ONE BATCH AT A TIME\n",
    "        Y_batch.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Y_hat = model(X_batch)\n",
    "        loss = loss_function(Y_hat, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(\"Final Loss:\",loss.data.item())\n",
    "print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
